{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13c0864f"
      },
      "source": [
        "To develop a Python-based speech-to-text system that converts spoken commands into text in real time, provides meaningful user feedback, handles errors gracefully, and allows comparison of different recognition methods.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------\n",
        "# 1Ô∏è‚É£ Install required packages and set up the environment\n",
        "#    (This part runs only once)\n",
        "# ------------------------------\n",
        "print(\"Setting up the environment...\")\n",
        "!pip uninstall whisper -y -q\n",
        "!pip install openai-whisper vosk SpeechRecognition pydub gradio -q\n",
        "!apt-get install -y ffmpeg -q\n",
        "\n",
        "import os\n",
        "import gradio as gr\n",
        "\n",
        "# Download the Vosk model if it doesn't exist\n",
        "if not os.path.exists(\"vosk-model-small-en-us-0.15\"):\n",
        "    print(\"Downloading Vosk model...\")\n",
        "    !wget https://alphacephei.com/vosk/models/vosk-model-small-en-us-0.15.zip -q\n",
        "    !unzip -q vosk-model-small-en-us-0.15.zip\n",
        "    print(\"Vosk model downloaded and unzipped.\")\n",
        "\n",
        "# ------------------------------\n",
        "# 2Ô∏è‚É£ Imports and Model Loading\n",
        "# ------------------------------\n",
        "import whisper\n",
        "from vosk import Model, KaldiRecognizer\n",
        "import speech_recognition as sr\n",
        "import wave\n",
        "import json\n",
        "from pydub import AudioSegment\n",
        "import traceback\n",
        "\n",
        "print(\"Loading models...\")\n",
        "# Load models once to improve performance\n",
        "try:\n",
        "    whisper_model = whisper.load_model(\"tiny\") # \"tiny\" for speed, \"base\" for better accuracy\n",
        "    vosk_model = Model(\"vosk-model-small-en-us-0.15\")\n",
        "    recognizer = sr.Recognizer()\n",
        "    print(\"Models loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading models: {e}\")\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# 3Ô∏è‚É£ The Core Transcription Function (as a Generator)\n",
        "# ------------------------------\n",
        "\n",
        "def format_results(whisper, vosk, google):\n",
        "    \"\"\"Helper function to format the text for display.\"\"\"\n",
        "    return (\n",
        "        f\"Whisper Output:\\n'{whisper}'\\n\\n\"\n",
        "        f\"Vosk Output:\\n'{vosk}'\\n\\n\"\n",
        "        f\"Google API Output:\\n'{google}'\"\n",
        "    )\n",
        "\n",
        "def transcribe_audio_generator(audio_path):\n",
        "    \"\"\"\n",
        "    Transcribes audio using three services, yielding results as they become available.\n",
        "    \"\"\"\n",
        "    # Initial state\n",
        "    whisper_text = \"Pending...\"\n",
        "    vosk_text = \"Pending...\"\n",
        "    google_text = \"Pending...\"\n",
        "    status_message = \"Starting...\"\n",
        "\n",
        "    # Immediately show the initial state\n",
        "    yield format_results(whisper_text, vosk_text, google_text), status_message\n",
        "\n",
        "    if audio_path is None:\n",
        "        yield format_results(\"Error\", \"Error\", \"Error\"), \"Error: No audio provided. Please record or upload.\"\n",
        "        return\n",
        "\n",
        "    # --- 1. Whisper Recognition ---\n",
        "    status_message = \"Recognizing with Whisper...\"\n",
        "    yield format_results(whisper_text, vosk_text, google_text), status_message\n",
        "    try:\n",
        "        result = whisper_model.transcribe(audio_path)\n",
        "        whisper_text = result[\"text\"].strip()\n",
        "        if not whisper_text:\n",
        "            whisper_text = \"Whisper could not understand audio.\"\n",
        "    except Exception as e:\n",
        "        whisper_text = f\"Whisper failed: {e}\"\n",
        "\n",
        "    status_message = \"Whisper complete. Starting Vosk...\"\n",
        "    yield format_results(whisper_text, vosk_text, google_text), status_message\n",
        "\n",
        "    # --- 2. Vosk Recognition ---\n",
        "    # Pre-processing: Convert audio to 16kHz mono WAV for Vosk\n",
        "    converted_for_vosk = \"vosk_temp.wav\"\n",
        "    try:\n",
        "        audio = AudioSegment.from_file(audio_path)\n",
        "        audio = audio.set_channels(1).set_frame_rate(16000)\n",
        "        audio.export(converted_for_vosk, format=\"wav\")\n",
        "\n",
        "        wf = wave.open(converted_for_vosk, \"rb\")\n",
        "        rec = KaldiRecognizer(vosk_model, wf.getframerate())\n",
        "        rec.SetWords(True)\n",
        "\n",
        "        while True:\n",
        "            data = wf.readframes(4000)\n",
        "            if len(data) == 0:\n",
        "                break\n",
        "            rec.AcceptWaveform(data)\n",
        "\n",
        "        res = json.loads(rec.FinalResult())\n",
        "        vosk_text = res.get(\"text\", \"\").strip()\n",
        "        if not vosk_text:\n",
        "            vosk_text = \"Vosk could not understand audio.\"\n",
        "\n",
        "    except Exception as e:\n",
        "        vosk_text = f\"Vosk failed: {e}\\n{traceback.format_exc()}\"\n",
        "    finally:\n",
        "        if os.path.exists(converted_for_vosk):\n",
        "            os.remove(converted_for_vosk)\n",
        "\n",
        "    status_message = \"Vosk complete. Starting Google API...\"\n",
        "    yield format_results(whisper_text, vosk_text, google_text), status_message\n",
        "\n",
        "    # --- 3. Google Speech Recognition API ---\n",
        "    try:\n",
        "        with sr.AudioFile(audio_path) as source:\n",
        "            audio_data = recognizer.record(source)\n",
        "        google_text = recognizer.recognize_google(audio_data).strip()\n",
        "        if not google_text:\n",
        "            google_text = \"Google API could not understand audio.\"\n",
        "    except sr.UnknownValueError:\n",
        "        google_text = \"Google API could not understand audio. Please try speaking more clearly.\"\n",
        "    except sr.RequestError:\n",
        "        google_text = \"Google API service is unavailable. Check your internet connection.\"\n",
        "    except Exception as e:\n",
        "        google_text = f\"Google API failed: {e}\"\n",
        "\n",
        "    # --- 4. Final Results and Status ---\n",
        "    is_successful = any(\n",
        "        \"could not understand\" not in text.lower() and \"failed\" not in text.lower()\n",
        "        for text in [whisper_text, vosk_text, google_text]\n",
        "    )\n",
        "\n",
        "    if is_successful:\n",
        "        status_message = \"Speech successfully converted to text!\"\n",
        "    else:\n",
        "        status_message = \"Speech recognition could not understand the audio. Please try again.\"\n",
        "\n",
        "    yield format_results(whisper_text, vosk_text, google_text), status_message\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# 4Ô∏è‚É£ Create the Gradio Interface\n",
        "# ------------------------------\n",
        "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
        "    gr.Markdown(\"# üé§ Speech to Text Comparison üìù\")\n",
        "    gr.Markdown(\"Upload an audio file or use your microphone. You will see the results appear one by one as they are processed.\")\n",
        "\n",
        "    with gr.Row():\n",
        "        audio_input = gr.Audio(\n",
        "            sources=[\"microphone\", \"upload\"],\n",
        "            type=\"filepath\",\n",
        "            label=\"Speak something or upload an audio file...\"\n",
        "        )\n",
        "\n",
        "    submit_button = gr.Button(\"Transcribe Audio\")\n",
        "\n",
        "    gr.Markdown(\"---\")\n",
        "    gr.Markdown(\"### Results\")\n",
        "\n",
        "    with gr.Row():\n",
        "        output_results = gr.Textbox(label=\"Comparative Results\", lines=10, interactive=False)\n",
        "        output_status = gr.Textbox(label=\"Status\", interactive=False)\n",
        "\n",
        "    # The .click() event will automatically handle the generator function,\n",
        "    # updating the outputs each time the function yields a value.\n",
        "    submit_button.click(\n",
        "        fn=transcribe_audio_generator,\n",
        "        inputs=audio_input,\n",
        "        outputs=[output_results, output_status]\n",
        "    )\n",
        "\n",
        "# Launch the app\n",
        "demo.launch(debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 871
        },
        "id": "h_stGYURsfAK",
        "outputId": "136b3107-cf1b-465a-ff69-61cd88003823"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up the environment...\n",
            "\u001b[33mWARNING: Skipping whisper as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mReading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 38 not upgraded.\n",
            "Loading models...\n",
            "Models loaded successfully.\n",
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://65c1e403abb9529006.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://65c1e403abb9529006.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7861 <> https://65c1e403abb9529006.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install whisper"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLmX8NV0qV5H",
        "outputId": "b9a6fa5b-c091-48a1-e49d-908785ba53da"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting whisper\n",
            "  Downloading whisper-1.1.10.tar.gz (42 kB)\n",
            "\u001b[?25l     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/42.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from whisper) (1.17.0)\n",
            "Building wheels for collected packages: whisper\n",
            "  Building wheel for whisper (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for whisper: filename=whisper-1.1.10-py3-none-any.whl size=41120 sha256=784022377c59695837cf4c7177ca430cfd709a33b1c095dac8a81e51059280f4\n",
            "  Stored in directory: /root/.cache/pip/wheels/34/b8/4e/9c4c3351d670e06746a340fb4b7d854c76517eec225e5b32b1\n",
            "Successfully built whisper\n",
            "Installing collected packages: whisper\n",
            "Successfully installed whisper-1.1.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install vosk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aAIEts-jqche",
        "outputId": "4629b198-9a19-4d8a-898c-2121e88fdf15"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting vosk\n",
            "  Downloading vosk-0.3.45-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from vosk) (2.0.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from vosk) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from vosk) (4.67.1)\n",
            "Collecting srt (from vosk)\n",
            "  Downloading srt-3.5.3.tar.gz (28 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: websockets in /usr/local/lib/python3.12/dist-packages (from vosk) (15.0.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->vosk) (2.23)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->vosk) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->vosk) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->vosk) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->vosk) (2025.8.3)\n",
            "Downloading vosk-0.3.45-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (7.2 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m63.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: srt\n",
            "  Building wheel for srt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for srt: filename=srt-3.5.3-py3-none-any.whl size=22427 sha256=da4763386640c6442e21005c10d5bd39a2180c9bc0e8105682b626c7f2e2d82e\n",
            "  Stored in directory: /root/.cache/pip/wheels/7e/75/5b/e1d5c3756631e4bda806f6cc9640153b39484bb6f7b0b8def3\n",
            "Successfully built srt\n",
            "Installing collected packages: srt, vosk\n",
            "Successfully installed srt-3.5.3 vosk-0.3.45\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install SpeechRecognition\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8EM-maLqiHW",
        "outputId": "2691c58e-ed71-4d84-95ef-aab0c2597166"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting SpeechRecognition\n",
            "  Downloading speechrecognition-3.14.3-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from SpeechRecognition) (4.15.0)\n",
            "Downloading speechrecognition-3.14.3-py3-none-any.whl (32.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m32.9/32.9 MB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: SpeechRecognition\n",
            "Successfully installed SpeechRecognition-3.14.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall whisper -y\n",
        "!pip install openai-whisper\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kb3uZ9vQqv4c",
        "outputId": "a5800ab9-f37d-4db7-d140-9c08d222ccde"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: whisper 1.1.10\n",
            "Uninstalling whisper-1.1.10:\n",
            "  Successfully uninstalled whisper-1.1.10\n",
            "Collecting openai-whisper\n",
            "  Downloading openai_whisper-20250625.tar.gz (803 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m803.2/803.2 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (10.8.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (2.0.2)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (0.11.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (2.8.0+cu126)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (4.67.1)\n",
            "Requirement already satisfied: triton>=2 in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (3.4.0)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.12/dist-packages (from triton>=2->openai-whisper) (75.2.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba->openai-whisper) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken->openai-whisper) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken->openai-whisper) (2.32.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (4.15.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (1.11.1.6)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2025.8.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->openai-whisper) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->openai-whisper) (3.0.2)\n",
            "Building wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20250625-py3-none-any.whl size=803979 sha256=342f70e1598f6ed7de522dc149d6e7d2c0b97f17c04d3f741124f9ba7cd2485d\n",
            "  Stored in directory: /root/.cache/pip/wheels/61/d2/20/09ec9bef734d126cba375b15898010b6cc28578d8afdde5869\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: openai-whisper\n",
            "Successfully installed openai-whisper-20250625\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "whisper"
                ]
              },
              "id": "f7eb1c9a286243fe952d36ddb6b1cfe2"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vZY0DpQhhArI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}